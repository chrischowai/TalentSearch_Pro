---
allowed-tools: all
description: Comprehensive pre-shipping validation across all platforms and environments
---

# ðŸš¢ /ship-it ARGUMENTS$

**ðŸš¨ CRITICAL PARALLEL EXECUTION REQUIREMENT:**
This command MUST spawn ALL agents IN PARALLEL using single messages with multiple Task tool invocations.
NEVER spawn agents sequentially - this defeats the purpose of comprehensive cross-platform validation.

## Arguments Integration

**Task**: ARGUMENTS$

Apply to: store_daddy: Store shipping context and platform requirements

**For any tasks created during validation, use scope: 'maintenance'**

---

## Cross-Platform Ship Readiness Validation

```
"Spawning agents IN PARALLEL for comprehensive ship readiness validation:
- murphy: Analyze configs, dependencies, and cross-platform compatibility
- validation: Test installation/execution on Windows/Mac/Linux/WSL environments  
- sherlock: Research platform-specific gotchas and deployment requirements
- opinion: Assess overall ship-readiness with platform confidence scores

Spawning all validation agents simultaneously now..."
[Use single message with multiple Task tool invocations]
```

**ðŸš¨ PARALLEL EXECUTION RULE:**
- NEVER: Spawn agent 1, wait for result, spawn agent 2...
- ALWAYS: Spawn ALL agents in ONE message with multiple tool calls
- Example: One message â†’ 4 Task tool invocations â†’ All run in parallel

## Agent Responsibilities

**ðŸ”§ murphy**: "Validate all configurations, dependencies, and technical setup for cross-platform compatibility. Check package.json dependencies, binary file coverage, path handling, build processes, and scan for mock data, hardcoded test values, or fake success responses."

**âœ… validation**: "Analyze code for cross-platform compatibility issues and simulate what would happen on Windows, Mac, Linux, and WSL. Review installation processes, binary compatibility, identify platform-specific failure points, and verify no mock/test data or hardcoded success responses remain in production code."

**ðŸ” sherlock**: "Research platform-specific issues, deployment requirements, and common gotchas for this project type. Identify compatibility risks and best practices."

**ðŸ“Š opinion**: "Provide GO/NO-GO shipping assessment with confidence scores for each platform. Categorize issues as Critical/Warning/Info with specific recommendations."

## Success Criteria

**YOU ARE NOT DONE UNTIL:**
- âœ… ALL platforms analyzed with confidence scores (>70% = realistic)
- âœ… NO critical blocking compatibility issues remain
- âœ… Installation process reviewed for cross-platform compatibility
- âœ… Dependencies properly declared/bundled for all platforms
- âœ… Cross-platform paths and configs validated
- âœ… Binary files exist for target architectures
- âœ… Potential platform-specific failure points identified
- âœ… NO mock data, test values, or hardcoded success responses in production code
- âœ… All API responses and data sources are real/production-ready

## Failure Response Protocol

**When platform issues are found:**
1. **IMMEDIATELY SPAWN AGENTS IN PARALLEL** to fix issues simultaneously
2. **FIX EVERYTHING** - address EVERY compatibility problem
3. **RE-VALIDATE** - run ship-it again after fixes
4. **REPEAT** - if new issues found, spawn more agents IN PARALLEL
5. **NO SHIPPING** - keep working until ALL platforms show âœ… GREEN

## Final Verification

- Analyze installation process for cross-platform compatibility
- Review binary file coverage and architecture support
- Confirm documentation matches actual requirements
- Store results and lessons learned with store_daddy

---

**Core principle: Never ship until it works for everyone, everywhere.**